
Session 8 HW

For one of my undergraduate independent research studies, we analyzed 232 written languages to see how close the words in each language are; due to time constraints and necessary alphabet consistency, we considered latin alphabet based languages. For further
research, it would be interesting to see how the project could be extended to more language alphabets by possibly training a model to work with languages that are not latin alphabet based, such as Korean or Chinese. The main issue is that in order for the languages to be compared, they must have a standard of comparison, so one way to achieve this is to take in the letter, symbol, or character and transform it into the international phonetic alphabet equivalent, and then all languages could be compared using the IPA. However, this will add a lot of complexity to the model because it will need to take in a data set of multiple languages, each with different alphabets, and then transform each word into its IPA equivalent, after which it is finally able to start performing statistical methods to compare how close the languages are. One important note is that we used the linguistic concept of a Swadesh list, which is a list of 207 concepts in each language, as the set of words to compare across languages. Due to the various layers, it will be best to use different models at different stages.For example, to transform the words into IPA, we could use the “epitran” library in Python. Then, we could use a ML model to check if the words were converted to IPA correctly. I would train the ML model on a data set using the IPA alphabet and a measure of success would be if all the words were correctly translated into IPA. Once this step is completed, we could start working on the statistical analysis. 